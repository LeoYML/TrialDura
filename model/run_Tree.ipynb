{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from preprocess.protocol_encode import protocol2feature, load_sentence_2_vec\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2vec = load_sentence_2_vec(\"../data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f'../data/time_prediction_train.csv', sep='\\t')\n",
    "test_data = pd.read_csv(f'../data/time_prediction_test.csv', sep='\\t')\n",
    "\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=0)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Handling\n",
    "train_data['criteria'].fillna('', inplace=True)\n",
    "valid_data['criteria'].fillna('', inplace=True)\n",
    "test_data['criteria'].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 32 sentences length can cover 95% of the data \n",
    "\n",
    "# criteria_lst = train_data['criteria']\n",
    "\n",
    "# in_criteria_lengths = []\n",
    "# ex_criteria_lengths = []\n",
    "\n",
    "# for criteria in criteria_lst:\n",
    "#     in_criteria, ex_criteria = protocol2feature(criteria, sentence2vec)\n",
    "#     in_criteria_lengths.append(len(in_criteria))\n",
    "#     ex_criteria_lengths.append(len(ex_criteria))\n",
    "\n",
    "# print(f\"Inclusion: {pd.Series(in_criteria_lengths).describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99, 0.999])}\")\n",
    "# print(f\"Exclusion: {pd.Series(ex_criteria_lengths).describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99, 0.999])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria2embedding(criteria_lst):\n",
    "    criteria_lst = [protocol2feature(criteria, sentence2vec) for criteria in criteria_lst]\n",
    "\n",
    "    incl_criteria = []\n",
    "    excl_criteria = []\n",
    "\n",
    "    for criteria in criteria_lst:\n",
    "        incl_criteria.append(torch.mean(criteria[0], dim=0))\n",
    "        excl_criteria.append(torch.mean(criteria[1], dim=0))\n",
    "\n",
    "    incl_emb = torch.stack(incl_criteria)\n",
    "    excl_emb = torch.stack(excl_criteria)\n",
    "\n",
    "    return torch.cat((incl_emb, excl_emb), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = criteria2embedding(train_data['criteria'])\n",
    "X_valid = criteria2embedding(valid_data['criteria'])\n",
    "X_test = criteria2embedding(test_data['criteria'])\n",
    "\n",
    "y_train = train_data['time_day']\n",
    "y_valid = valid_data['time_day']\n",
    "y_test = test_data['time_day']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT\n",
    "# params = {\n",
    "#     'boosting': 'gbdt',\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'learning_rate': 0.01,\n",
    "#     'early_stopping_round': 10,\n",
    "#     'verbosity': 1,\n",
    "#     'max_depth': 10,\n",
    "#     'num_threads': 4\n",
    "# }\n",
    "\n",
    "# RF\n",
    "params = {\n",
    "    'boosting': 'rf',\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'early_stopping_round': 10,\n",
    "    'verbosity': 1,\n",
    "    'max_depth': 10,\n",
    "    'num_threads': 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=1000, valid_sets=[lgb_eval], callbacks=[lgb.log_evaluation()])\n",
    "\n",
    "# Predict on test data\n",
    "print(gbm.best_iteration)\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "pearson_score, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f'The RMSE of prediction is: {rmse}')\n",
    "print(f'The MAE of prediction is: {mae}')\n",
    "print(f'The R2 of prediction is: {r2}')\n",
    "print(f'The Pearson Score of prediction is: {pearson_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    # 'bagging_fraction': 0.8,\n",
    "    # 'feature_fraction': 0.8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'early_stopping_round': 10,\n",
    "    'verbosity': 1,\n",
    "    # 'max_depth': 10,\n",
    "    'num_threads': 4,\n",
    "    'device':'gpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n",
    "xgb_reg = xgb.train(params, xgb_train, num_boost_round=100, evals=[(xgb_eval, 'eval')], early_stopping_rounds=10)\n",
    "\n",
    "# Predict on test data\n",
    "print(xgb_reg.best_iteration)\n",
    "y_pred = gbm.predict(xgb.DMatrix(X_test), iteration_range=(0, gbm.best_iteration))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "pearson_score, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f'The RMSE of prediction is: {rmse}')\n",
    "print(f'The MAE of prediction is: {mae}')\n",
    "print(f'The R2 of prediction is: {r2}')\n",
    "print(f'The Pearson Score of prediction is: {pearson_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n",
    "xgb_reg = xgb.train(params, xgb_train, num_boost_round=100, evals=[(xgb_eval, 'eval')], early_stopping_rounds=10)\n",
    "\n",
    "# Predict on test data\n",
    "print(xgb_reg.best_iteration)\n",
    "y_pred = gbm.predict(xgb.DMatrix(X_test), iteration_range=(0, gbm.best_iteration))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "pearson_score, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f'The RMSE of prediction is: {rmse}')\n",
    "print(f'The MAE of prediction is: {mae}')\n",
    "print(f'The R2 of prediction is: {r2}')\n",
    "print(f'The Pearson Score of prediction is: {pearson_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_reg = AdaBoostRegressor(n_estimators=50, learning_rate=0.5)\n",
    "adaboost_reg = adaboost_reg.fit(X_train, y_train)\n",
    "y_pred = adaboost_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "pearson_score, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f'The RMSE of prediction is: {rmse}')\n",
    "print(f'The MAE of prediction is: {mae}')\n",
    "print(f'The R2 of prediction is: {r2}')\n",
    "print(f'The Pearson Score of prediction is: {pearson_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "pearson_score, _ = pearsonr(y_test, y_pred)\n",
    "\n",
    "print(f'The RMSE of prediction is: {rmse}')\n",
    "print(f'The MAE of prediction is: {mae}')\n",
    "print(f'The R2 of prediction is: {r2}')\n",
    "print(f'The Pearson Score of prediction is: {pearson_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
